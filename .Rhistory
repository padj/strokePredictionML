model$nnet <- caret::train(as.factor(stroke) ~ .,
data = train,
method = "nnet",
metric = "Kappa",
trControl = model$ctrl)
print(model$nnet) # Review the model summary
# Apply the model to the test set
predicted <- predict(model$nnet, test)
# Calculate the Confusion Matrix and statistics surrounding the performance of
# our model
model$CM <- caret::confusionMatrix(data = predicted,
reference = as.factor(test$stroke),
positive='1')
print(model$CM) # view confusion matrix
# It's also common to score the performance of the model using the area under
# the ROC curve metric (AUC). We can calculate the ROC directly
ROC_val = pROC::roc(response=test$stroke, predictor=ordered(predicted))
print(ROC_val) # This can be plotted also
# Or just spit out the AUC metric
AUC_val = pROC::auc(response=test$stroke, predictor=ordered(predicted))
print(AUC_val) # A score of 1 is very good, a score of 0.5 is very bad.
model$AUC <- AUC_val
# Add train and test file names for reference and reproducibility.
model$train_file <- train_file
model$test_file <- test_file
# Now that the model is developed/trained, we should save the model, along with
# information about it for use later.
# The list "model" contains the trained model, the control file, the
# confusion matrix and the AUC metric, as well as the names of the train and
# test files used in development of the model.
output_location <- paste0('scripts/MLModels/','nnet_model_selected.RData')
save(model, file=output_location)
# nnet_model
###############################################################################
# -----------------------------------------------------------------------------
# SCRIPT:
# Name:       nnet_model.R
# Date:       23 June 2021
# Version:    1.0.0
# Authors:    thomas.padgett
#
# Description:
#             Imports prepped, feature-engineered test and train data. Trains
#             a neural network, tested on the test set. Performance is reviewed
#             using the confusion matrix and AUC metric.
#
# Change notes:
#             N/A
#
# -----------------------------------------------------------------------------
###############################################################################
#### Preamble ####
set.seed(10) # Ensure repeatability
library(pROC)
library(caret)
library(e1071) #required within caret::train()
#### Function definitions ####
#### Import data ####
train_file <- "data/trainDataOversampled_featEng_selected_v1.csv"
test_file <- "data/testDataOversampled_featEng_selected_v1.csv"
train <- read.csv(train_file)
test <- read.csv(test_file)
#### Model ####
model <- list()
# Define the training control method.
# K-fold cross validation (number = folds)
model$ctrl <- caret::trainControl(method = "repeatedcv",
number = 20,
repeats = 5,
search = "grid",
classProbs = FALSE)
# Train the neural network
model$nnet <- caret::train(as.factor(stroke) ~ .,
data = train,
method = "nnet",
metric = "Kappa",
trControl = model$ctrl)
print(model$nnet) # Review the model summary
# Apply the model to the test set
predicted <- predict(model$nnet, test)
# Calculate the Confusion Matrix and statistics surrounding the performance of
# our model
model$CM <- caret::confusionMatrix(data = predicted,
reference = as.factor(test$stroke),
positive='1')
print(model$CM) # view confusion matrix
# It's also common to score the performance of the model using the area under
# the ROC curve metric (AUC). We can calculate the ROC directly
ROC_val = pROC::roc(response=test$stroke, predictor=ordered(predicted))
print(ROC_val) # This can be plotted also
# Or just spit out the AUC metric
AUC_val = pROC::auc(response=test$stroke, predictor=ordered(predicted))
print(AUC_val) # A score of 1 is very good, a score of 0.5 is very bad.
model$AUC <- AUC_val
# Add train and test file names for reference and reproducibility.
model$train_file <- train_file
model$test_file <- test_file
# Now that the model is developed/trained, we should save the model, along with
# information about it for use later.
# The list "model" contains the trained model, the control file, the
# confusion matrix and the AUC metric, as well as the names of the train and
# test files used in development of the model.
output_location <- paste0('scripts/MLModels/','nnet_model_selected.RData')
save(model, file=output_location)
# rf_model
###############################################################################
# -----------------------------------------------------------------------------
# SCRIPT:
# Name:       rf_model.R
# Date:       24 June 2021
# Version:    1.0.0
# Authors:    thomas.padgett
#
# Description:
#             Imports prepped, feature-engineered test and train data. Trains
#             a random forest, tested on the test set. Performance is reviewed
#             using the confusion matrix and AUC metric.
#
# Change notes:
#             N/A
#
# -----------------------------------------------------------------------------
###############################################################################
#### Preamble ####
set.seed(10) # Ensure repeatability
library(pROC)
library(caret)
library(e1071) #required within caret::train()
# We'll use this later to define where we output to.
output_location <- 'output/rf/'
#### Function definitions ####
#### Import data ####
train_file <- "data/trainDataOversampled_featEng_selected_v1.csv"
test_file <- "data/testDataOversampled_featEng_selected_v1.csv"
train <- read.csv(train_file)
test <- read.csv(test_file)
#### Model ####
model <- list()
mtry <- sqrt(ncol(train))
tuneGrid <- expand.grid(.mtry=mtry)
modellist <- list()
# Define the training control method.
# K-fold cross validation (number = folds)
model$ctrl <- caret::trainControl(method = "repeatedcv",
number = 10,
repeats = 3,
search = "grid",
classProbs = FALSE)
# Train the random forest
# with different ntree parameters
for (ntree in c(10,50,100,250,500,1000,2000,2500,3500,4000,5000)) {
rf <- caret::train(as.factor(stroke) ~ .,
data = train,
method = "rf",
metric = "Kappa",
tuneGrid = tuneGrid,
trControl = model$ctrl)
modellist[[toString(ntree)]] <- rf
}
# This bit allows us to plot and assess the different models
results <- resamples(modellist)
summary(results)
# We can now save the plot
image_name <- 'rf_dotplot'
png(paste0(output_location,image_name,'.png'), width = 800, height = 600)
dotplot(results)
dev.off()
# Review the dotplot and select the model that performs best. In  this case,
# the version with nTree = 500 performs well (although all perform well).
# Set the best version as the final version
model$rf <- modellist$'500'
# Apply the model to the test set
predicted <- predict(model$rf, test)
# Calculate the Confusion Matrix and statistics surrounding the performance of
# our model
model$CM <- caret::confusionMatrix(data = predicted,
reference = as.factor(test$stroke),
positive='1')
print(model$CM) # view confusion matrix
# It's also common to score the performance of the model using the area under
# the ROC curve metric (AUC). We can calculate the ROC directly
ROC_val = pROC::roc(response=test$stroke, predictor=ordered(predicted))
print(ROC_val) # This can be plotted also
# Or just spit out the AUC metric
AUC_val = pROC::auc(response=test$stroke, predictor=ordered(predicted))
print(AUC_val) # A score of 1 is very good, a score of 0.5 is very bad.
model$AUC <- AUC_val
# Add train and test file names for reference and reproducibility.
model$train_file <- train_file
model$test_file <- test_file
# Now that the model is developed/trained, we should save the model, along with
# information about it for use later.
# The list "model" contains the trained model, the control file, the
# confusion matrix and the AUC metric, as well as the names of the train and
# test files used in development of the model.
output_model_name <- 'rf_model_selected_noAge.RData'
save(model, file=paste0(output_location,output_model_name))
# The real question is why does this perform so well on the training data and
# so badly on the test data?
# The answer is likely that the random forest is overfitted to the training
# data.
# dataPrep_selection
###############################################################################
# -----------------------------------------------------------------------------
# SCRIPT:
# Name:       dataPrep_selection.R
# Date:       23 June 2021
# Version:    1.0.0
# Authors:    thomas.padgett
#
# Description:
#             Variation on dataPrep to select most important variables based
#             on varImp of initial nnet and rf models.
#
# Change notes:
#             N/A
#
# -----------------------------------------------------------------------------
###############################################################################
#### Preamble ####
set.seed(1)
library(modeest)
library(imbalance)
library(caret)
#### Function definitions ####
#### Import data ####
dataLoc = "data/healthcare-dataset-stroke-data.csv"
data <- read.csv(dataLoc)
data_orig <- data #back up of original data
#### Data cleaning ####
# As in dataPrep.R
data$bmi <- as.numeric(data$bmi)
totalMissing <- colSums(is.na(data))
avgBMI <- round(mean(na.omit(data$bmi)),1)
data$bmi <- as.numeric(lapply(data$bmi, function(x) {if (is.na(x))
{x=avgBMI} else {x=x}}))
rm(avgBMI)
avgSmokeS <- modeest::mfv(data$smoking_status)
data$smoking_status <- lapply(data$smoking_status, function(x) {if
(x=="Unknown") {x=avgSmokeS} else {x=x}})
rm(avgSmokeS)
data$gender <- factor(x=data$gender, levels=unique(data$gender))
data$ever_married <- factor(x = data$ever_married,
levels = unique(data$ever_married))
data$work_type <- factor(x = data$work_type,
levels = unique(data$work_type))
data$Residence_type <- factor(x = data$Residence_type,
levels = unique(data$Residence_type))
data$smoking_status <- factor(x = data$smoking_status,
levels = unique(data$smoking_status))
#### Feature Engineering ####
# As in dataPrep.R
data$male <- sapply(data$gender, function(x) {if (x=="Male") {x=1} else {x=0}})
data$female <- sapply(data$gender, function(x) {if (x=="Female")
{x=1} else {x=0}})
data <- data[-2]
# And similar for other factor categories:
data$ever_married_yes <- sapply(data$ever_married, function(x) {if (x=="Yes")
{x=1} else {x=0}})
data$ever_married_no <- sapply(data$ever_married, function(x) {if (x=="No")
{x=1} else {x=0}})
data <- data[-5]
data$smoking_status_smokes <- sapply(data$smoking_status,
function(x) {if (x=="smokes")
{x=1} else {x=0}})
data$smoking_status_never_smoked <- sapply(data$smoking_status,
function(x) {if (x=="never smoked")
{x=1} else {x=0}})
data$smoking_status_formerly_smoked <- sapply(data$smoking_status,
function(x) {if
(x=="formerly smoked") {x=1} else {x=0}})
data <- data[-9]
data$residence_type_rural <- sapply(data$Residence_type,
function(x) {if (x=="Rural") {x=1} else {x=0}})
data$residence_type_urban <- sapply(data$Residence_type,
function(x) {if (x=="Urban") {x=1} else {x=0}})
data <- data[-6]
data$work_type_self_employed <- sapply(data$work_type,
function(x) {if (x=="Self-employed")
{x=1} else {x=0}})
data$work_type_private <- sapply(data$work_type, function(x) {if (x=="Private")
{x=1} else {x=0}})
data$work_type_govt <- sapply(data$work_type, function(x) {if (x=="Govt_job")
{x=1} else {x=0}})
data$work_type_child <- sapply(data$work_type, function(x) {if (x=="children")
{x=1} else {x=0}})
data$work_type_never_worked <- sapply(data$work_type, function(x)
{if (x=="Never_worked") {x=1} else {x=0}})
data <- data[-5]
# Then convert nums to ints:
data$male <- as.integer(data$male)
data$female <- as.integer(data$female)
data$ever_married_yes <- as.integer(data$ever_married_yes)
data$ever_married_no <- as.integer(data$ever_married_no)
data$smoking_status_smokes <- as.integer(data$smoking_status_smokes)
data$smoking_status_never_smoked<- as.integer(data$smoking_status_never_smoked)
data$smoking_status_formerly_smoked <- as.integer(data$smoking_status_formerly_smoked)
data$residence_type_rural <- as.integer(data$residence_type_rural)
data$residence_type_urban <- as.integer(data$residence_type_urban)
data$work_type_self_employed <- as.integer(data$work_type_self_employed)
data$work_type_private <- as.integer(data$work_type_private)
data$work_type_govt <- as.integer(data$work_type_govt)
data$work_type_child <- as.integer(data$work_type_child)
data$work_type_never_worked <- as.integer(data$work_type_never_worked)
View(data)
# Removing age and rerunning both rf and nnet
data <- within(data, rm(ever_married_no))
# Removing age and rerunning both rf and nnet
data <- within(data, rm(ever_married_no))
data <- within(data, rm(work_type_child, work_type_never_worked))
data <- within(data, rm(male))
data <- within(data, rm(residence_type_rural))
#### Oversampling ####
# ML algorithms need a balanced dataset, meaning a relatively equal number of
# true and false stroke patients. We can use the imbalanceRatio function within
# the imbalance package to assess the balance of the data
print(imbalance::imbalanceRatio(data, 'stroke')) # shows that only 5% of the
# patients suffered stroke, therefore not balanced.
# We can use the imbalance package to oversample the data, effectively creating
# new patients that suffered stroke to balance the dataset.
#First shuffle data for good mix of factors at all parts of data set
shuffle_index <- sample(1:nrow(data))
data <- data[shuffle_index, ]
# Oversampling should only be applied to data used for training NOT testing, and
# therefore we need to split the data into test and train sets first
# (80/20 split)
splits <- sample(1:2, size=nrow(data), prob=c(0.8,0.2), replace=TRUE)
train <- data[splits==1,]
test <- data[splits==2,]
train <- train[,-1] # removed id
# oversample the data (1:1 ratio)
train_oversampled <- imbalance::oversample(train,
classAttr = "stroke",
ratio = 1,
method = "MWMOTE")
# Check the imbalance of each set
print(imbalance::imbalanceRatio(test, 'stroke'))
print(imbalance::imbalanceRatio(train, 'stroke'))
print(imbalance::imbalanceRatio(train_oversampled, 'stroke'))
#### Output data ####
write.csv(train_oversampled, 'data/trainDataOversampled_featEng_selected_v1.csv', row.names=FALSE)
write.csv(test, 'data/testDataOversampled_featEng_selected_v1.csv', row.names=FALSE)
# nnet_model
###############################################################################
# -----------------------------------------------------------------------------
# SCRIPT:
# Name:       nnet_model.R
# Date:       23 June 2021
# Version:    1.0.0
# Authors:    thomas.padgett
#
# Description:
#             Imports prepped, feature-engineered test and train data. Trains
#             a neural network, tested on the test set. Performance is reviewed
#             using the confusion matrix and AUC metric.
#
# Change notes:
#             N/A
#
# -----------------------------------------------------------------------------
###############################################################################
#### Preamble ####
set.seed(10) # Ensure repeatability
library(pROC)
library(caret)
library(e1071) #required within caret::train()
#### Function definitions ####
#### Import data ####
train_file <- "data/trainDataOversampled_featEng_selected_v1.csv"
test_file <- "data/testDataOversampled_featEng_selected_v1.csv"
train <- read.csv(train_file)
test <- read.csv(test_file)
#### Model ####
model <- list()
# Define the training control method.
# K-fold cross validation (number = folds)
model$ctrl <- caret::trainControl(method = "repeatedcv",
number = 20,
repeats = 5,
search = "grid",
classProbs = FALSE)
# Train the neural network
model$nnet <- caret::train(as.factor(stroke) ~ .,
data = train,
method = "nnet",
metric = "Kappa",
trControl = model$ctrl)
print(model$nnet) # Review the model summary
# Apply the model to the test set
predicted <- predict(model$nnet, test)
# Calculate the Confusion Matrix and statistics surrounding the performance of
# our model
model$CM <- caret::confusionMatrix(data = predicted,
reference = as.factor(test$stroke),
positive='1')
print(model$CM) # view confusion matrix
# It's also common to score the performance of the model using the area under
# the ROC curve metric (AUC). We can calculate the ROC directly
ROC_val = pROC::roc(response=test$stroke, predictor=ordered(predicted))
print(ROC_val) # This can be plotted also
# Or just spit out the AUC metric
AUC_val = pROC::auc(response=test$stroke, predictor=ordered(predicted))
print(AUC_val) # A score of 1 is very good, a score of 0.5 is very bad.
model$AUC <- AUC_val
# Add train and test file names for reference and reproducibility.
model$train_file <- train_file
model$test_file <- test_file
# Now that the model is developed/trained, we should save the model, along with
# information about it for use later.
# The list "model" contains the trained model, the control file, the
# confusion matrix and the AUC metric, as well as the names of the train and
# test files used in development of the model.
output_location <- paste0('scripts/MLModels/','nnet_model_selected.RData')
save(model, file=output_location)
# rf_model
###############################################################################
# -----------------------------------------------------------------------------
# SCRIPT:
# Name:       rf_model.R
# Date:       24 June 2021
# Version:    1.0.0
# Authors:    thomas.padgett
#
# Description:
#             Imports prepped, feature-engineered test and train data. Trains
#             a random forest, tested on the test set. Performance is reviewed
#             using the confusion matrix and AUC metric.
#
# Change notes:
#             N/A
#
# -----------------------------------------------------------------------------
###############################################################################
#### Preamble ####
set.seed(10) # Ensure repeatability
library(pROC)
library(caret)
library(e1071) #required within caret::train()
# We'll use this later to define where we output to.
output_location <- 'output/rf/'
#### Function definitions ####
#### Import data ####
train_file <- "data/trainDataOversampled_featEng_selected_v1.csv"
test_file <- "data/testDataOversampled_featEng_selected_v1.csv"
train <- read.csv(train_file)
test <- read.csv(test_file)
#### Model ####
model <- list()
mtry <- sqrt(ncol(train))
tuneGrid <- expand.grid(.mtry=mtry)
modellist <- list()
# Define the training control method.
# K-fold cross validation (number = folds)
model$ctrl <- caret::trainControl(method = "repeatedcv",
number = 10,
repeats = 3,
search = "grid",
classProbs = FALSE)
# Train the random forest
# with different ntree parameters
for (ntree in c(10,50,100,250,500,1000,2000,2500,3500,4000,5000)) {
rf <- caret::train(as.factor(stroke) ~ .,
data = train,
method = "rf",
metric = "Kappa",
tuneGrid = tuneGrid,
trControl = model$ctrl)
modellist[[toString(ntree)]] <- rf
}
# This bit allows us to plot and assess the different models
results <- resamples(modellist)
summary(results)
# We can now save the plot
image_name <- 'rf_dotplot'
png(paste0(output_location,image_name,'.png'), width = 800, height = 600)
dotplot(results)
dev.off()
# Review the dotplot and select the model that performs best. In  this case,
# the version with nTree = 500 performs well (although all perform well).
# Set the best version as the final version
model$rf <- modellist$'500'
# Apply the model to the test set
predicted <- predict(model$rf, test)
# Calculate the Confusion Matrix and statistics surrounding the performance of
# our model
model$CM <- caret::confusionMatrix(data = predicted,
reference = as.factor(test$stroke),
positive='1')
print(model$CM) # view confusion matrix
# It's also common to score the performance of the model using the area under
# the ROC curve metric (AUC). We can calculate the ROC directly
ROC_val = pROC::roc(response=test$stroke, predictor=ordered(predicted))
print(ROC_val) # This can be plotted also
# Or just spit out the AUC metric
AUC_val = pROC::auc(response=test$stroke, predictor=ordered(predicted))
print(AUC_val) # A score of 1 is very good, a score of 0.5 is very bad.
model$AUC <- AUC_val
# Add train and test file names for reference and reproducibility.
model$train_file <- train_file
model$test_file <- test_file
# Now that the model is developed/trained, we should save the model, along with
# information about it for use later.
# The list "model" contains the trained model, the control file, the
# confusion matrix and the AUC metric, as well as the names of the train and
# test files used in development of the model.
output_model_location <- 'scripts/MLModels/'
output_model_name <- 'rf_model_selected.RData'
save(model, file=paste0(output_model_location,output_model_name))
# The real question is why does this perform so well on the training data and
# so badly on the test data?
# The answer is likely that the random forest is overfitted to the training
# data.
